{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n",
      "arn:aws:iam::856970663410:role/EcoAI-SageMaker_Role\n",
      "2.3.1\n",
      "s3.Bucket(name='ecoai-fws-invasives')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ecoai-fws-invasives/Modelling/',\n",
       " 'ecoai-fws-invasives/Modelling/test_dataset.csv',\n",
       " 'ecoai-fws-invasives/Modelling/train_dataset.csv']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import IPython\n",
    "import json\n",
    "import sys\n",
    "import s3fs\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "source_bucket_name = \"ecoai-fws-invasives\"\n",
    "source_bucket_prefix = \"Modelling/\"\n",
    "source_bucket = s3.Bucket(source_bucket_name)\n",
    "prefix = 'EcoAI-Invasives'\n",
    "os.environ[\"AWS_REGION\"] = region\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "print(tf.__version__)\n",
    "print(source_bucket)\n",
    "\n",
    "# To List 5 files in your accessible bucket\n",
    "fs.ls('s3://ecoai-fws-invasives/Modelling/')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 's3://' + source_bucket_name + '/' + source_bucket_prefix\n",
    "#print(train_file_path)\n",
    "val_file_path = 's3://' + source_bucket_name + '/' + source_bucket_prefix\n",
    "\n",
    "train_dataset = pd.read_csv(train_file_path + 'test_dataset.csv')\n",
    "#test_dataset = pd.read_csv(val_file_path + 'train_dataset.csv')\n",
    "\n",
    "train_y = train_dataset[['anhb','herb','litter','sage','shrub','bare']].copy()\n",
    "train_X = train_dataset.drop(['anhb','herb','litter','sage','shrub','bare'], axis=1)\n",
    "train_X_2 = RobustScaler(quantile_range=(25, 75)).fit_transform(train_X)\n",
    "train_X = pd.DataFrame(train_X_2, columns = train_X.columns)\n",
    "print(train_X)\n",
    "#test_y = test_dataset[['anhb','herb','litter','sage','shrub','bare']].copy()\n",
    "#test_X = test_dataset.drop(['anhb','herb','litter','sage','shrub','bare'], axis=1)\n",
    "#test_X_2 = RobustScaler(quantile_range=(25, 75)).fit_transform(test_X)\n",
    "#test_X = pd.DataFrame(test_X_2, columns = test_X.columns)\n",
    "#print(test_X)\n",
    "\n",
    "#Shapes to pass to get_modl\n",
    "n_inputs = train_X.shape[1]\n",
    "n_outputs =  train_y.shape[1]\n",
    "print(n_inputs)\n",
    "print(n_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
